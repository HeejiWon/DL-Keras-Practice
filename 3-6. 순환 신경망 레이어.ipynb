{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. RNN(Recurrent Neural Network)\n",
    "cf) https://wikidocs.net/22886<br/>\n",
    "    https://www.edwith.org/deeplearningchoi/lecture/15840/\n",
    "#### 1) RNN\n",
    "- RNN은 시퀀스 모델(입력과 출력을 시퀀스 단위로 처리하는 모델)<br/>\n",
    "cf) 재귀 신경망(Recursive NN)과는 전혀 다른 개념\n",
    "<img src=\"img/3_1.png\" width=\"300\" height=\"300\">\n",
    "\n",
    "\n",
    "- 메모리 셀(초록색 셀)은 이전의 값을 기억하려고 하는 __메모리 역할__\n",
    "- 메모리 셀이 출력층 방향으로 또는 다음 시점 t+1의 자신에게 보내는 값(재귀적)을<br/> __은닉상태(hidden state)__라고 함\n",
    "\n",
    "\n",
    "- 수식\n",
    "<img src=\"img/3_2.png\" width=\"400\" height=\"300\">\n",
    "<img src=\"img/3_3.png\" width=\"500\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow import keras"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.layers import SimpleRNN"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# RNN 층을 추가하는 코드\n",
    "\n",
    "model.add(SimpleRNN(hidden_size, input_shape=(timesteps, input_dim)))\n",
    "model.add(SimpleRNN(hidden_size, input_length=M, input_dim=N)) # 같은 코드"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "- hidden_size($D_h$) : 은닉 상태의 크기(output_dim과도 동일). 중소형 모델의 경우, 보통 128, 256, 512, 1024 등의 값을 가짐\n",
    "- timesteps($d$) : 입력 시퀀스의 길이(input_length라고도 함)\n",
    "- input_dim : 입력의 크기(dimensionality of word representation)\n",
    "- 위 코드는 은닉층만을 위한 코드임(출력층 x)\n",
    "<img src=\"img/3_4.png\" width=\"100\" height=\"300\">"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 출력의 형태\n",
    "- return_sequences 옵션<br/>\n",
    "    ○ 출력값이 (batch_size, output_dim) : 최종 시점의 은닉 상태만을 리턴(왼쪽 그림)<br/>\n",
    "    ○ 출력값이 (batch_size, timesteps, output_dim) : 은닉 상태들의 전체 시퀀스 리턴(오른쪽 그림)\n",
    "    cf) batch_size는 hidden state의 개수\n",
    "<img src=\"img/3_5.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.python.keras.models import Sequential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_1\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_1 (SimpleRNN)     (None, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 출력값이 (batch_size=2, output_dim=10) \n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, input_shape=(2,10)))\n",
    "# model.add(SimpleRNN(3, input_length=2, input_dim=10))와 동일함.\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Model: \"sequential_3\"\n",
      "_________________________________________________________________\n",
      "Layer (type)                 Output Shape              Param #   \n",
      "=================================================================\n",
      "simple_rnn_3 (SimpleRNN)     (8, 2, 3)                 42        \n",
      "=================================================================\n",
      "Total params: 42\n",
      "Trainable params: 42\n",
      "Non-trainable params: 0\n",
      "_________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "# 출력값이 (batch_size=8, timesteps=2, output_dim=10)\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(3, batch_input_shape=(8,2,10), return_sequences=True))\n",
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Deep RNN\n",
    "\n",
    "model = Sequential()\n",
    "model.add(SimpleRNN(hidden_size, return_sequences = True))\n",
    "model.add(SimpleRNN(hidden_size, return_sequences = True))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 양방향 RNN (Birectional RNN)\n",
    "- 양방향 RNN은 기본적으로 두 개의 메모리 셀을 사용  \n",
    ": __앞 시점의 은닉상태(Forward States), 뒤 시점의 은닉상태(Backward States)__\n",
    "<img src=\"img/3_6.png\" width=\"300\" height=\"300\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from tensorflow.keras.models import Sequential\n",
    "from tensorflow.keras.layers import SimpleRNN, Bidirectional"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 은닉층 하나 더 추가\n",
    "\n",
    "model = Sequential()\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True), input_shape=(timesteps, input_dim)))\n",
    "model.add(Bidirectional(SimpleRNN(hidden_size, return_sequences = True)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. 긴 시퀀스를 기억할 수 있는 LSTM (Long Short-Term Memory units) 레이어\n",
    "- __장기 의존성 문제(the problem of Long-Term Dependencies)__  \n",
    ": SimpleRNN(Vanilla RNN)은 시점(time step)이 길어질수록 앞의 정보가 뒤로 충분히 전달되지 못하는 문제\n",
    "<img src=\"img/3_7.png\" width=\"450\" height=\"300\"><br/>\n",
    "- LSTM은 은닉층의 메모리 셀에 __입력 게이트, 망각 게이트, 출력 게이트__를 추가하여 불필요한 기억을 지우고, 기억해야할 것들을 정함\n",
    "- __Cell State__($C_t$) : t시점의 셀 상태\n",
    "\n",
    "\n",
    "- $W_{xi},W_{xg},W_{xf},W_{xo}$ : $x_t$와 함께 각 게이트에서 사용되는 4개의 가중치\n",
    "- $W_{hi},W_{hg},W_{hf},W_{ho}$ : $h_{t−1}$와 함께 각 게이트에서 사용되는 4개의 가중치\n",
    "- $b_i,b_g,b_f,b_o$ : 각 게이트에서 사용되는 4개의 편향"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 1) 입력 게이트\n",
    "- 현재 정보를 기억하기 위한 게이트\n",
    "-     $i_t=\\sigma(W_{xi}x_t+W_{hi}h_{t-1}+b_i)$<br/>\n",
    "- $g_t=tanh(W_{xg}x_t+W_{hg}h_{t-1}+b_g)$\n",
    "<img src=\"img/3_8.png\" width=\"300\">\n",
    "\n",
    "    \n",
    "    \n",
    "- $i_t, g_t$ : 현재 시점의 $x_t$와 이전 시점의 은닉 상태 $h_t$의 가중합을 각각 시그모이드 함수와 하이퍼볼릭탄젠트 함수를 지나 출력되는 값 \n",
    "- $i_t$ : 시그모이드 함수를 지나 0~1사이의 값 출력  \n",
    "=> $g_t$를 얼마나(%) 반영할 지"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2) 삭제 게이트\n",
    "- 기억을 삭제하기 위한 게이트  \n",
    "- $f_t=\\sigma(W_{xf}x_t+W_{hf}h_{t-1}+b_f)$\n",
    "<img src=\"img/3_9.png\" width=\"300\">\n",
    "    \n",
    "    \n",
    "    \n",
    "- $f_t$ : 현재 시점의 $x_t$와 이전 시점의 은닉 상태 $h_t$의 가중합을 시그모이드 함수를 지나 출력되는 값 (0~1 => 얼마나 반영할 지)  \n",
    "=> 삭제 과정을 거친 정보의 양 (0에 가까울수록 정보 많이 삭제)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 3) 셀 상태(장기 상태)\n",
    "- $C_t=f_t∘C_{t−1}+i_t∘g_t$  (∘는 원소별 곱을 의미)\n",
    "<img src=\"img/3_10.png\" width=\"300\">\n",
    "\n",
    "- 삭제 게이트는 이전 시점의 입력을 얼마나 반영할지를 의미하고, 입력 게이트는 현재 시점의 입력을 얼마나 반영할지를 결정함"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 4) 출력 게이트와 은닉 상태(단기 상태)\n",
    "- $o_t=\\sigma(W_{xo}x_t+W_{ho}h_{t-1}+b_o)$ \n",
    "- $h_t=o_t∘tanh(c_t)$\n",
    "<img src=\"img/3_11.png\" width=\"300\">\n",
    "- $o_t$ : 현재 시점 t의 x값과 이전 시점 t-1의 은닉 상태가 시그모이드 함수를 지난 값<br/> \n",
    "    => 현재 시점 t의 은닉 상태(단기 상태, $h_t$)를 얼마나 빼낼 지 결정\n",
    "- 장기 상태($c_t$)의 값(tanh -> -1~1 값)과 출력 게이트의 값이 연산되면서, 값이 걸러지는 효과가 발생하여 은닉상태가 됨"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. 게이트 순환 유닛 (Gated Recurrent Unit, GRU)\n",
    "- GRU는 LSTM의 장기 의존성 문제에 대한 해결책을 유지하면서, 은닉 상태를 업데이트 하는 계산을 줄임 (구조 간단히)\n",
    "- 속도는 빠르지만, 비슷한 성능 (데이터가 적으면 매개 변수의 양이 적은 GRU, 많으면 LSTM)\n",
    "- (LSTM은 출력, 입력, 삭제 게이트) -> __업데이트 게이트와 리셋 게이트__만 존재\n",
    "<img src='img/3_13.png' width='400'>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
